# -*- coding: utf-8 -*-
"""Home_Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OmPW0E3_JF98U3Mzsp2uhLe5u46aEyzU

# Q1: Implementing a Basic Autoencoder

## a. Import Necessary Libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist

"""## b. Load and Preprocess the Dataset"""

# Load the MNIST dataset
(x_train, _), (x_test, _) = mnist.load_data()

# Normalize the images to the range [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten the images to 784-dimensional vectors (28x28 = 784)
x_train = x_train.reshape((x_train.shape[0], 784))
x_test = x_test.reshape((x_test.shape[0], 784))

"""## c. Define the Autoencoder Model"""

# Define the encoder
input_img = tf.keras.Input(shape=(784,))
encoded = layers.Dense(32, activation='relu')(input_img)
#encoded = layers.Dense(16, activation='relu')(input_img)
#encoded = layers.Dense(64, activation='relu')(input_img)

# Define the decoder
decoded = layers.Dense(784, activation='sigmoid')(encoded)

# Build the autoencoder model
autoencoder = models.Model(input_img, decoded)

# Compile the model
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

"""## d. Train the Model"""

# Train the autoencoder
autoencoder.fit(
    x_train, x_train,
    epochs=20,
    batch_size=256,
    shuffle=True,
    validation_data=(x_test, x_test)
)

"""## e. Visualizing Reconstruction"""

# Predict the test images using the trained autoencoder
decoded_imgs = autoencoder.predict(x_test)

# Plot original and reconstructed images
n = 10  # Number of images to display
plt.figure(figsize=(20, 4))
for i in range(n):
    # Display original images
    plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    plt.axis('off')

    # Display reconstructed images
    plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')
    plt.axis('off')

plt.show()

"""## f. Modify Latent Dimension Size (Experimentation)

##For latent dimension size = 16
encoded = layers.Dense(16, activation='relu')(input_img)

##For latent dimension size = 64
encoded = layers.Dense(64, activation='relu')(input_img)

**Observation**:

Latent Size = 16: Reconstruction will likely be more blurry and less accurate.

Latent Size = 32 (default): Good balance of compression and reconstruction quality.

Latent Size = 64: Likely to have the best reconstruction quality, but with reduced compression.

# Q2: Implementing a Denoising Autoencoder

## Step 1: Modify the Data Preprocessing Step
"""

import numpy as np

# Adding Gaussian noise
noise_factor = 0.5
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)

# Clipping the values to be between 0 and 1
x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)

# Visualize some of the noisy images
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(1, n, i + 1)
    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')
    plt.axis('off')
plt.show()

"""## Step 2: Define the Denoising Autoencoder Model"""

# Define the encoder
input_img = tf.keras.Input(shape=(784,))
encoded = layers.Dense(32, activation='relu')(input_img)

# Define the decoder
decoded = layers.Dense(784, activation='sigmoid')(encoded)

# Build the autoencoder model
denoising_autoencoder = models.Model(input_img, decoded)

# Compile the model
denoising_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

"""## Step 3: Train the Model"""

denoising_autoencoder.fit(
    x_train_noisy, x_train,
    epochs=20,
    batch_size=256,
    shuffle=True,
    validation_data=(x_test_noisy, x_test)
)

"""## Step 4: Visualize the Denoising Results"""

# Predict the test images using the trained denoising autoencoder
decoded_imgs = denoising_autoencoder.predict(x_test_noisy)

# Plot original, noisy, and reconstructed images
n = 10  # Number of images to display
plt.figure(figsize=(30, 6))
for i in range(n):
    # Display original images
    plt.subplot(3, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    plt.title("Original")
    plt.axis('off')

    # Display noisy images
    plt.subplot(3, n, i + 1 + n)
    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')
    plt.title("Noisy")
    plt.axis('off')

    # Display reconstructed images
    plt.subplot(3, n, i + 1 + 2*n)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')
    plt.title("Reconstructed")
    plt.axis('off')

plt.show()

"""# **Observations**

The reconstructed images are significantly cleaned compared to the noisy ones.

The model is effectively learning to remove noise and restore the original digit shapes.

Some details are slightly blurred, but the main structure of the digits is preserved well.

# Q3: Implementing an RNN for Text Generation

## Step 1: Import Necessary Libraries
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import string

"""## Step 2: Load & Prepare Text Dataset"""

# Load text data
path_to_file = tf.keras.utils.get_file("shakespeare.txt", "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')

# Print the first 1000 characters of the text
print(text[:1000])

text = text.lower()  # Convert to lowercase

"""## Step 3: Preprocessing Text Data"""

# Create a unique character list (including punctuation)
chars = sorted(set(text))
vocab_size = len(chars)
print(f"Total unique characters: {vocab_size}")

# Creating a mapping from characters to integers
char_to_index = {char: index for index, char in enumerate(chars)}
index_to_char = {index: char for index, char in enumerate(chars)}

# Convert text to integer representation
text_as_int = np.array([char_to_index[c] for c in text])

# Define sequence length and create training sequences
seq_length = 100
examples_per_epoch = len(text) // (seq_length + 1)

# Create training dataset
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)

# Function to split input and target text
def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

# Apply the function to split sequences
dataset = sequences.map(split_input_target)

"""## Step 4: Build the LSTM Model"""

# Batch size and buffer size for shuffling
BATCH_SIZE = 64
BUFFER_SIZE = 10000

# Create training batches
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# Define the model (Use the same architecture you used before)
model = tf.keras.Sequential([
    layers.Embedding(vocab_size, 256, input_length=seq_length),
    layers.LSTM(512, return_sequences=True),
    layers.LSTM(512, return_sequences=True),
    layers.Dense(vocab_size, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Train the model
EPOCHS = 20
model.fit(dataset, epochs=EPOCHS)

"""## Step 5: Generate Text With Punctuation"""

def generate_text(model, start_string, temperature=1.0, num_chars=1000):
    input_eval = [char_to_index[s] for s in start_string]
    input_eval = tf.expand_dims(input_eval, 0)
    text_generated = []

    for _ in range(num_chars):
        predictions = model(input_eval)
        predictions = tf.squeeze(predictions, 0)

        # Use temperature to adjust randomness of predictions
        predictions = predictions / temperature
        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()

        input_eval = tf.expand_dims([predicted_id], 0)
        text_generated.append(index_to_char[predicted_id])

    return (start_string + ''.join(text_generated))

print(generate_text(model, start_string="to be, or not to be", temperature=0.5))

"""# Q4: Sentiment Classification Using RNN

## Step 1: Import Necessary Libraries
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from sklearn.metrics import confusion_matrix, classification_report

"""# Step 2: Load and Preprocess the Dataset"""

# Load the dataset (only keep the top 10,000 most common words)
max_features = 10000
maxlen = 200  # Cut reviews after this number of words

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

# Pad sequences to ensure uniform input size
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

print(f"Training samples: {len(x_train)}")
print(f"Testing samples: {len(x_test)}")

"""## Step 3: Build the LSTM Model"""

model = tf.keras.Sequential([
    layers.Embedding(max_features, 128, input_length=maxlen),
    layers.LSTM(128, return_sequences=True),
    layers.LSTM(128),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

"""## Step 4: Train the Model"""

epochs = 10
batch_size = 64

history = model.fit(
    x_train, y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=0.2
)

"""## Step 5: Evaluate the Model"""

# Evaluate on the test dataset
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

"""## Step 6: Generate Classification Report and Confusion Matrix"""

# Generate predictions
y_pred = (model.predict(x_test) > 0.5).astype("int32")

# Generate classification report
print("Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=["Negative", "Positive"]))

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

