{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Loading the Dataset & Preparing the Data"
      ],
      "metadata": {
        "id": "EsUw46woO6jR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5lEAb6ZMXjT",
        "outputId": "b74f96f8-ee83-4bfd-c2a6-4e29cace18cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Length of text: 1115394 characters\n",
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (Shakespeare Sonnets)\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read the text\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "\n",
        "# Get unique characters in the dataset\n",
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print(f'{vocab_size} unique characters')\n",
        "\n",
        "# Create a mapping from characters to numbers\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Convert text to integers (character indices)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Maximum length of sequences for training\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "# Create training examples\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "# Create sequences of the desired length\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "# Split input and target\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Creating training batches\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define & Train the Model"
      ],
      "metadata": {
        "id": "2r2wqvD0PDGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Train the model\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCRqKxNoMkWe",
        "outputId": "7887fac7-c660-46b0-ec09-b71e1ae05628"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - loss: 2.8849\n",
            "Epoch 2/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - loss: 1.8657\n",
            "Epoch 3/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - loss: 1.6046\n",
            "Epoch 4/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 67ms/step - loss: 1.4803\n",
            "Epoch 5/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - loss: 1.4048\n",
            "Epoch 6/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 69ms/step - loss: 1.3530\n",
            "Epoch 7/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 69ms/step - loss: 1.3064\n",
            "Epoch 8/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 70ms/step - loss: 1.2733\n",
            "Epoch 9/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 1.2381\n",
            "Epoch 10/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 1.2069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model weights\n",
        "model.save_weights('text_generator.weights.h5')\n"
      ],
      "metadata": {
        "id": "45ZMawO2Nboe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reload the Model for Text Generation"
      ],
      "metadata": {
        "id": "XLzAfuoTOmz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Recreate the model architecture for inference\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Build the model with batch size = 1 for inference\n",
        "model.build(tf.TensorShape([1, None]))  # This is where the input shape is defined properly\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_weights('text_generator.weights.h5')\n"
      ],
      "metadata": {
        "id": "rzOGN0rVNcRn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Text Generation Function\n"
      ],
      "metadata": {
        "id": "nE_WNNRYOTNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, temperature=1.0):\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Convert start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store results\n",
        "    text_generated = []\n",
        "\n",
        "    # Reset states for stateful LSTM (Accessing LSTM layer directly)\n",
        "    model.layers[1].reset_states()  # Resetting only the LSTM layer, which is at index 1\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Apply temperature\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        # Sample from the distribution\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Ensure the predicted_id is within range\n",
        "        if predicted_id >= vocab_size:\n",
        "            predicted_id = np.random.randint(0, vocab_size)\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        # Append the predicted character to the generated text\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n"
      ],
      "metadata": {
        "id": "Dw2cHeJiNeSE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Generating Text"
      ],
      "metadata": {
        "id": "Le48DRZHOQ6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text(model, start_string=\"ROMEO: \", temperature=0.5)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqxB1OIRNg7E",
        "outputId": "ac80104a-68a0-48d0-ce4e-7e0b9c673f24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: stand to her so far for the sea,\n",
            "As she was nothing in the best of the presence and the back way\n",
            "To the actions of the maid of England;\n",
            "And hanging them not the shadow of his face,\n",
            "And that you shall not stay away the ground\n",
            "From the power of the shadow of your father,\n",
            "Richard the recompense of the world,\n",
            "And give him thanks for him; which was the heart of me\n",
            "That bare me still be too much. Well, I'll go to thee,\n",
            "That stays here being so much a cousin\n",
            "That I have fair words, and the poor devil's dear days\n",
            "Had natural marriage which you have\n",
            "sent for this prophecian that makes a thing,\n",
            "While I did well be the fire of heaven,\n",
            "That live shall not wear the state of the heart, and the great sorrow\n",
            "And bring my father, and the storms did\n",
            "Than the precious crown is straited\n",
            "Which never been drinks it off,\n",
            "And that the world is scarce the field to seek him that\n",
            "But that I shall be past to my dear lord;\n",
            "But let the streets to the king and stands and prove\n",
            "As mine eyes me with me a more than me,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aC7safp8NjZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}